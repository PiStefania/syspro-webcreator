Ονοματεπώνυμο: Στεφανία Πάτσου
Α.Μ.: 1115201400156

Μεταγλώττιση Προγράμματος: make στους φακέλους webserver, webcreator

Εκτέλεση Προγράμματος: 	π.χ. 1) ./webcreator.sh root_directory dataset.txt 3 3
							 2) ./myhttpd -p 8080 -c 9090 -t 4 -d ../webcreator/root_directory
							 3) ./mycrawler -h 192.168.1.10 -p 8080 -c 7070 -t 4 -d ./save_dir http://192.168.1.10:8080/site0/page0_24040.html

Δοκιμή Προγράμματος:
-Το πρόγραμμα εκτελείται επιτυχώς σε λειτουργικό linux.

Δομές:
Για webserver:
-generalInfo.h: Περιέχει μία δομή που κρατά πληροφορίες που αφορούν την εντολή STATS, όπως milliseconds και έναρξη χρόνου σε seconds, αριθμός σελίδων και byte που στάλθηκαν συνολικά.
-threadPool.h: Περιέχει μία δομή την οποία μοιράζονται τα νήματα, η οποία περιέχει έναν πίνακα από int και μεταβλητές που αφορούν το μέγεθος του. Επίσης, περιέχει μία δομή στην οποία συμπεριμαβάνονται γενικές πληροφορίες για την κλήση της συνάρτησης του νήματος αλλά και τα mutexes, condition variables, που βοηθούν στον συγχρονισμό των νημάτων.

Για webcrawler:
-generalInfo.h: Περιέχει μία δομή που κρατά πληροφορίες που αφορούν την εντολή STATS, όπως milliseconds και έναρξη χρόνου σε seconds, αριθμός σελίδων και byte που λήφθηκαν συνολικά.
-threadPool.h: Περιέχει μία δομή στην οποία συμπεριμαβάνονται γενικές πληροφορίες για την κλήση της συνάρτησης του νήματος αλλά και τα mutexes, condition variables, που βοηθούν στον συγχρονισμό των νημάτων και την ουρά από την οποία λαμβάνουν μία τιμή τα threads.
-linksQueue.h: Περιέχει τρείς δομές, μία που περιγράφει τον κάθε κόμβο της ουράς, την δομή της ουράς και μία δομή με πίνακα από strings, στην οποία εισάγονται links, μόνο αν δεν βρίσκονται ήδη μέσα στην ουρά.

Υλοποίηση:
Για webcreator:	Ο φάκελος webcreator περιλαμβάνει 3 αρχεία, ένα αρχείο dataset.txt από το οποίο παίρνει γραμμές το shell script για να δημιουργήσει τα αρχεία στον φάκελο root_directory και το ίδιο το webcreator.sh shell script. Ο webcreator.sh έχει υλοποιηθεί με πολλές συναρτήσεις οι οποίες ενώνονται και εκτελούν συνολικά τα προαπαιτούμενα. Συγκεκριμένα, περιέχει συναρτήσεις οι οποίες ελέγχουν τα ορίσματα του χρήστη, αν αυτά είναι ορθά, έπειτα δημιουργεί τους φακέλους και διαγράφει τους ήδη υπάρχοντες, τοποθετεί την κάθε σελίδα σε έναν πίνακα, υπολογίζει για κάθε σελίδα τις τιμές όπως k, m, f, p και οι υπολειπόμενες συναρτήσεις συνδυάζονται έτσι ώστε να δημιουργηθούν καταλλήλως τα html αρχεία.

Για webserver:
-main.c: ελέγχει τα ορίσματα και δημιουργεί τυχόν δομές. Έπειτα χειρίζεται τα sockets.
-generalInfo.c: στο συγκεκριμένο αρχείο υπολογίζονται τα στατιστικά για την εντολή STATS.
-httpRequests.c: ελέγχει και δημιουργεί το response από τον server στον client.
-variousMethods.c: βοηθητικές ερωτήσεις για την λειτουργία του προγράμματος. Συγκεκριμένα, περιέχει την readGetLinesFromCrawler στην οποία διαβάζει το request και στέλνει πίσω την απάντηση, την createManageSockets στην οποία μέσα σε μία while(1) παίρνει ως είσοδο είτε το command είτε το server socket και τοποθετεί το fd του socket στον πίνακα από το οποίο θα λάβουν τα threads, ενώ κάνει signal ότι ο πίνακας δεν είναι κενός.
-threadPool.c: περιέχει συναρτήσεις για τα νήματα. Η acceptHandler τρέχει έως ότου ο πίνακας δεν είναι κενός, παίρνει το πρώτο στοιχείο του και το στέλνει για να διαβάσει από αυτό. Έπειτα κάνει signal πως ο πίνακας δεν είνα γεμάτος. Οι insertPoolData και getPoolData είναι οι κύριες συναρτήσεις για εισαγωγή και εξαγωγή fd.

Για webcrawler:
-main.c: ελέγχει τα ορίσματα και δημιουργεί τυχόν δομές. Έπειτα χειρίζεται εισάγει το αρχικό λινκ στην ουρά και αρχίζει να χειρίζεται τα sockets.
-generalInfo.c: στο συγκεκριμένο αρχείο υπολογίζονται τα στατιστικά για την εντολή STATS αλλά και επιστρέφονται τα αποτελέσματα από την SEARCH. Την SEARCH την υλοποιώ κάνοντας fork και execv αφού πριν έχω φτιάξει ένα αρχείο "docfile" με τα μονοπάτια των φακέλων το οποίο το δίνω ως όρισμα στο execv. Οι default workers που δημιουργούνται είναι 10. Έπειτα αντικαθιστώ το stdin/stdout με δύο pipes έτσι ώστε τα αποτελέσματα να ληφθούν από τον webcrawler. Τα pipes για το stdout του jobexecutor είναι NON_BLOCK οπότε και τα αποτελέσματα ελέγχονται μέσω poll.
-linksQueue.c: περιέχει συναρτήσεις για την ουρά. Περιέχει επίσης ένα array στο οποίο εισάγονται λινκς έτσι ώστε να μην ξαναεισαχθούν στην ουρά. Τα συγκεκριμένα εισάγονται με insertionSort(Ο(1)) και κάνω αναζήτηση με binarySearch(Ο(Ν/2)).
-threadPool.c: περιέχει συναρτήσεις για τα νήματα. Η connectHandler τρέχει έως ότου η ουρά δεν είναι κενή, συνδέεται με ένα socket και στέλνει το συνδεόμενο socket για να δώσει το request και να πάρει το response. Οι insertQueue και popFromQueue είναι οι κύριες συναρτήσεις για εισαγωγή και εξαγωγή linkNode.
-variousMethods.c: βοηθητικές ερωτήσεις για την λειτουργία του προγράμματος. Συγκεκριμένα, περιέχει την readGetLinesFromServer στην οποία στέλνει το request και λαμβάνει πίσω την απάντηση, την createManageSockets στην οποία μέσα σε μία while(1) παίρνει ως είσοδο το command.

Σημειώσεις:
-Ο webcreator εκτελείται μόνο αν w,p >= 1.
-Στον webcreator, αν μία σελίδα είναι μόνη της, δεν περιέχει λινκ (δεν περιέχει τον εαυτό της).
-Για εκτύπωση των πληροφοριών στο terminal, χρησιμοποιώ το stderr, διότι το stdout χρησιμοποιείται ως ανακατεύθυνση για τα αρχεία.
-Για το poll στην εκτέλεση του SEARCH command, έχω βάλει ώς default τιμή milliseconds τα POLL_MILLI 2000.
-Το save_dir στον webcrawler αδειάζει όταν καλείται ο webcrawler.
-Η hostname -I είναι βοηθητική για να βρούμε την διεύθυνση IP.
-Στο listen ενός socket έχω ως default τιμή το LISTEN_SIZE 128.
-Και στα δύο προγράμματα ελέγχεται αν το server port είναι ίδιο με το command port, αν είναι, η εκτέλεση του προγράμματος τελειώνει.
-Ο webcrawler δημιουργεί ένα docfile για να δώσει ως όρισμα στον jobExecutor.
-Ο jobExecutor καλείται με την execv.
-Το Makefile του κάνει make και το Makefile του jobExecutor.
-Το SEARCH command έχει ώς default deadline 100 seconds.
-Το startingUrl πρέπει να έχει μπροστά τον host.
-Το content-length είναι όσο και το HTML body.
-Η κλήση του webcreator.sh με όρισμα έναν φάκελο, πρέπει το όρισμα να μην έχει τον τελευταίο χαρακτήρα '/'. Το ίδιο συμβαίνει και γενικά για ορίσματα που ζητούν φακέλους.
-Στον webcrawler, το free των threads γίνεται με pthread_kill, ενώ στον webserver, το free γίνεται με pthread_cancel -> pthread_join.
-Η δημιουργία των socket και η διαχείριση των pool, έγινε με την καθοδήγηση των διαφανειών του μαθήματος.
-Για το redirect των stdin/stdout του jobExecutor, συμβουλεύτηκα την σελίδα: https://stackoverflow.com/questions/20187734/c-pipe-to-stdin-of-another-program
-Τα threads περιμένουν 3 seconds(sleep(3)) έτσι ώστε η δημιουργία του socket τους να είναι επιτυχής.